import streamlit as st
import pandas as pd
from google.oauth2 import service_account
from googleapiclient.discovery import build
import time
import json

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
SCOPES = ['https://www.googleapis.com/auth/webmasters.readonly']
DEFAULT_PROPERTY = "sc-domain:cable.ru"  # –ò–∑–º–µ–Ω–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ

st.set_page_config(page_title="Google Indexing Checker", page_icon="üîç", layout="wide")
st.title("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ URL –≤ Google")

# === –ü–æ–ª—É—á–µ–Ω–∏–µ —É—á—ë—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Streamlit Secrets ===
try:
    credentials_info = st.secrets["google_service_account"]
    credentials = service_account.Credentials.from_service_account_info(
        credentials_info, scopes=SCOPES
    )
    webmasters = build('searchconsole', 'v1', credentials=credentials)
except Exception as e:
    st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Google Search Console API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Secrets.")
    st.stop()

# === –§—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–¥–Ω–æ–≥–æ URL ===
def inspect_url(url, property_url):
    try:
        request_body = {"inspectionUrl": url, "siteUrl": property_url}
        response = webmasters.urlInspection().index().inspect(body=request_body).execute()
        inspection_result = response.get('inspectionResult', {})
        if not inspection_result:
            return {"indexed": False, "error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ—Ç API"}
        
        verdict = inspection_result.get('inspectionResult', {}).get('verdict')
        coverage = inspection_result.get('inspectionResult', {}).get('coverageState', 'UNKNOWN')
        last_crawl = inspection_result.get('inspectionResult', {}).get('lastCrawlTime', '‚Äî')
        google_canonical = inspection_result.get('inspectionResult', {}).get('googleCanonical', '‚Äî')
        
        return {
            "indexed": verdict == "PASS",
            "coverage_state": coverage,
            "last_crawl_date": last_crawl,
            "gsc_canonical": google_canonical,
            "error": ""
        }
    except Exception as e:
        return {"indexed": False, "error": str(e)}

# === –í–≤–æ–¥ URL ===
st.subheader("üì• –í–≤–µ–¥–∏—Ç–µ URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")

input_method = st.radio("–°–ø–æ—Å–æ–± –≤–≤–æ–¥–∞:", ["–í—Ä—É—á–Ω—É—é", "–ß–µ—Ä–µ–∑ Excel-—Ñ–∞–π–ª"], horizontal=True)

urls = []
if input_method == "–í—Ä—É—á–Ω—É—é":
    urls_input = st.text_area("–í–≤–µ–¥–∏—Ç–µ URL –ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫–µ", height=150)
    if urls_input.strip():
        urls = [u.strip() for u in urls_input.strip().split("\n") if u.strip().startswith("http")]
else:
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel-—Ñ–∞–π–ª —Å –∫–æ–ª–æ–Ω–∫–æ–π 'URL'", type=["xlsx", "xls"])
    if uploaded_file:
        try:
            df = pd.read_excel(uploaded_file)
            if "URL" not in df.columns:
                st.error("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º 'URL'")
            else:
                urls = df["URL"].dropna().astype(str).tolist()
                urls = [u for u in urls if u.startswith("http")]
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ ===
if urls:
    st.info(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.")
    
    property_url = st.text_input("URL —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –≤ Search Console", value=DEFAULT_PROPERTY)
    delay = st.slider("–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫)", 1, 5, 2)
    
    if st.button("üöÄ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –≤ Google", type="primary"):
        if not property_url.startswith(("http", "sc-domain:")):
            st.error("URL —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 'http://' –∏–ª–∏ 'sc-domain:'")
        else:
            progress = st.progress(0)
            status = st.empty()
            results = {}

            for i, url in enumerate(urls):
                status.text(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ {i+1}/{len(urls)}: {url}")
                res = inspect_url(url, property_url)
                results[url] = res
                time.sleep(delay)
                progress.progress((i + 1) / len(urls))

            # === –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
            st.success("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            indexed = sum(1 for r in results.values() if r["indexed"])
            total = len(results)
            st.metric("–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ", f"{indexed} –∏–∑ {total} ({indexed/total*100:.1f}%)")
            
            # –¢–∞–±–ª–∏—Ü–∞
            data = []
            for url, res in results.items():
                if res["error"]:
                    status_text = f"‚ùå –û—à–∏–±–∫–∞: {res['error']}"
                else:
                    status_text = "‚úÖ –î–∞" if res["indexed"] else "‚ùå –ù–µ—Ç"
                data.append({
                    "URL": url,
                    "–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω": status_text,
                    "–ü–æ–∫—Ä—ã—Ç–∏–µ": res["coverage_state"],
                    "–ü–æ—Å–ª–µ–¥–Ω–∏–π –∫—Ä–∞—É–ª": res["last_crawl_date"],
                    "–ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π URL": res["gsc_canonical"]
                })
            
            df_results = pd.DataFrame(data)
            st.dataframe(df_results, use_container_width=True)
            
            # –≠–∫—Å–ø–æ—Ä—Ç
            csv = df_results.to_csv(index=False, encoding="utf-8-sig")
            st.download_button("üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)", csv, "google_indexing_results.csv", "text/csv")

st.markdown("---")
st.caption("üí° –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π Google Search Console API. –í—Å–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ—á–Ω—ã –∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã.")